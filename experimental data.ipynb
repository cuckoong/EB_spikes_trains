{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5xT5gyZzcXd4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.io\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sods model and standard nb-gml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective function, calculate the maximum likelihood for SOD model\n",
    "def S0D_sum(x0, K, y, x):\n",
    "    r=x0[0]\n",
    "    b0=x0[1]\n",
    "    loggam=x0[2]\n",
    "    s=x0[3]\n",
    "    b=x0[4:]\n",
    "    b = np.append([b0],b)\n",
    "    gam = np.exp(loggam)\n",
    "    \n",
    "    y = y.reshape(1,-1)\n",
    "    x_tmp = np.transpose(np.hstack([np.ones((x.shape[0],1)), x]))\n",
    "    tmp = b.reshape(1,-1).dot(x_tmp)\n",
    "    mu = (gam*np.exp(tmp)+1)**(-1/gam)\n",
    "    \n",
    "    import scipy.special as sc\n",
    "    LL_sum =sc.gammaln(r+s*mu)+sc.gammaln(y+s-s*mu)+ \\\n",
    "          sc.gammaln(r+y)+sc.gammaln(s)-\\\n",
    "          sc.gammaln(r+y+s)-sc.gammaln(r)-\\\n",
    "          sc.gammaln(s*mu)-sc.gammaln(s-s*mu)-sc.gammaln(y+1)\n",
    "    \n",
    "    LL_sum = -1*np.sum(LL_sum)\n",
    "    return LL_sum\n",
    "\n",
    "\n",
    "#objective function, calculate the maximum likelihood for NBGLM model\n",
    "def NBGLM_sum(x0, K, y, x):\n",
    "    r=x0[0]\n",
    "    b0=x0[1]\n",
    "    loggam=x0[2]\n",
    "    b=x0[4:]\n",
    "    gam = np.exp(loggam)\n",
    "    b = np.append([b0],b)\n",
    "    \n",
    "    y = y.reshape(1,-1)\n",
    "    x_tmp = np.transpose(np.hstack([np.ones((x.shape[0],1)), x]))\n",
    "    tmp = b.reshape(1,-1).dot(x_tmp)\n",
    "    mu = (gam*np.exp(tmp)+1)**(-1/gam)\n",
    "  \n",
    "    \n",
    "    import scipy.special as sc\n",
    "    LL_sum=sc.gammaln(r+y)-sc.gammaln(y+1)-sc.gammaln(r)- \\\n",
    "        r/gam*np.log(gam*np.exp(tmp)+1)+y*np.log(1-mu)\n",
    "    LL_sum = -1*np.sum(LL_sum)\n",
    "    return LL_sum\n",
    "\n",
    "\n",
    "\n",
    "#objective function, calculate the maximum likelihood for poission model\n",
    "def Poisson_sum(x0, K, y, x):\n",
    "    b0=x0[1]\n",
    "    b=x0[4:]\n",
    "    b = np.append([b0],b)\n",
    "    \n",
    "    y = y.reshape(1,-1)\n",
    "    x_tmp = np.transpose(np.hstack([np.ones((x.shape[0],1)), x]))\n",
    "    mu = b.reshape(1,-1).dot(x_tmp)\n",
    "    lam = np.exp(mu)\n",
    "  \n",
    "    import scipy.special as sc\n",
    "    LL_sum=y*mu-lam-sc.gammaln(y+1)\n",
    "    LL_sum = -1*np.sum(LL_sum)\n",
    "    return LL_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 fold cross validation, repeat 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = dict()\n",
    "filename = '62814.mat'\n",
    "spike_count = scipy.io.loadmat(filename)['spike_count']\n",
    "spike_count = spike_count.T\n",
    "\n",
    "\n",
    "for i in range(spike_count.shape[1]):\n",
    "    #preparing training and test data for each neuron\n",
    "    y = spike_count[:,i]\n",
    "    X = np.delete(spike_count, i, axis=1)\n",
    "\n",
    "    from sklearn.model_selection import RepeatedKFold\n",
    "    n_splits = 5\n",
    "    rkf = RepeatedKFold(n_splits=n_splits, n_repeats = 10, random_state = 42)\n",
    "    \n",
    "    llr_arr = np.zeros((n_splits,3))  \n",
    "    result[str(i)] = dict()\n",
    "    j = 0\n",
    "    for train_index, test_index in rkf.split(X):\n",
    "    #     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        #value initiation\n",
    "        r = 5\n",
    "        b0 = -5\n",
    "        loggam = 5\n",
    "        s = 20\n",
    "        M = spike_count.shape[1]\n",
    "        K = spike_count.shape[0]\n",
    "        rng42 = np.random.default_rng(seed=42)\n",
    "        b = rng42.uniform(-1,1,M-1)\n",
    "        x0 = [r,b0,loggam,s]\n",
    "        x0.extend(b)\n",
    "\n",
    "        bnds_sod = [[1,None],[None,None],[None,None],[1,None]]\n",
    "        bnds_sod.extend([[-1,1]]*(M-1))\n",
    "\n",
    "        # 3 models\n",
    "        op_sod = scipy.optimize.minimize(S0D_sum, x0, args=(K, y_train, X_train), \\\n",
    "                                     bounds = bnds_sod, method='SLSQP',options = {'maxiter':200})\n",
    "        try:\n",
    "            op_sod.success\n",
    "            \n",
    "        except ValueError:\n",
    "            print('not converge in sod model')       \n",
    "\n",
    "        op_nbglm = scipy.optimize.minimize(NBGLM_sum, x0, args=(K, y_train, X_train), \\\n",
    "                                     bounds = bnds_sod, method='SLSQP',options = {'maxiter':200})\n",
    "        try:\n",
    "            op_nbglm.success\n",
    "            \n",
    "        except ValueError:\n",
    "            print('not converge in nbglm model')\n",
    "        \n",
    "        op_poisson = scipy.optimize.minimize(Poisson_sum, x0, args=(K, y_train, X_train), \\\n",
    "                                     bounds = bnds_sod, method='SLSQP',options = {'maxiter':200})\n",
    "        try:\n",
    "            op_poisson.success      \n",
    "        except ValueError:\n",
    "            print('not converge in poisson model')\n",
    "            \n",
    "        #save model    \n",
    "        result[str(i)]['op_sod_'+str(j)]=op_sod\n",
    "        result[str(i)]['op_nbglm_'+str(j)]=op_nbglm\n",
    "        result[str(i)]['op_poisson_'+str(j)]=op_poisson   \n",
    "\n",
    "        #load weights\n",
    "        w_sod = op_sod['x']\n",
    "        w_nbglm = op_nbglm['x']\n",
    "        w_poi = op_poisson['x']\n",
    "        #save weights\n",
    "        result[str(i)]['w_sod_'+str(j)]=w_sod\n",
    "        result[str(i)]['w_nbglm_'+str(j)]=w_nbglm\n",
    "        result[str(i)]['w_poi_'+str(j)]=w_poi  \n",
    "\n",
    "        #log-likelihood\n",
    "        ll_sod = -1*S0D_sum(w_sod, K, y_test, X_test)\n",
    "        ll_ngblm = -1*NBGLM_sum(w_nbglm, K, y_test, X_test)\n",
    "        ll_poi = -1*Poisson_sum(w_poi, K, y_test, X_test)\n",
    "        \n",
    "        result[str(i)]['ll_sod_'+str(j)]=ll_sod\n",
    "        result[str(i)]['ll_ngblm_'+str(j)]=ll_ngblm\n",
    "        result[str(i)]['ll_poi_'+str(j)]=ll_poi\n",
    "\n",
    "        sod_nb = (ll_sod-ll_ngblm)/np.abs(ll_ngblm)\n",
    "        sod_poi = (ll_sod-ll_poi)/np.abs(ll_poi)\n",
    "        \n",
    "        print(ll_sod,ll_ngblm,ll_poi)\n",
    "        j = j+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "XhRttGFdUBEL",
    "outputId": "ab7157b2-9563-443a-9077-b97cd8f1183c"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "file_id = filename+'_experiment_data.pkl'\n",
    "\n",
    "# Saving the objects:\n",
    "with open(file_id, 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump(result, f, protocol=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting back the objects:\n",
    "with open(file_id,'rb') as f:  # Python 3: open(..., 'rb')\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ll_sod=[]\n",
    "ll_ngblm=[]\n",
    "ll_poi=[]\n",
    "len_neuron = len(data)\n",
    "for neuron_id in range(len_neuron):\n",
    "    for i in range(50): #5-fold cross-validation, repeat 10 times\n",
    "        ll_sod.append(data[str(neuron_id)]['ll_sod_'+str(i)])\n",
    "        ll_ngblm.append(data[str(neuron_id)]['ll_ngblm_'+str(i)])\n",
    "        ll_poi.append(data[str(neuron_id)]['ll_poi_'+str(i)])\n",
    "\n",
    "        \n",
    "neuron_id = [[i]*50 for i in range(len_neuron)]\n",
    "neuron_id = [i for j in neuron_id for i in j]\n",
    "\n",
    "df = {'neuron_id':neuron_id, 'll_sod':ll_sod,'ll_ngblm':ll_ngblm,'ll_poi':ll_poi}\n",
    "#data frame for all log-likelihood\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "df['sod_nbglm'] = (df['ll_sod']-df['ll_ngblm'])/np.abs(df['ll_ngblm'])\n",
    "df['sod_poi'] = (df['ll_sod']-df['ll_poi'])/np.abs(df['ll_poi'])\n",
    "df.to_csv(filename[:-4]+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "spike_counts.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
